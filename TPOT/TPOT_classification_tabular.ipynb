{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ce3d6364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "import pandas as pd\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "60c531e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 0.12.0 of tpot is outdated. Version 0.12.1 was released Tuesday August 15, 2023.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing missing values in feature set\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/140 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.8478752118511078\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.8478752118511078\n",
      "\n",
      "Generation 3 - Current best internal CV score: 0.8478752118511078\n",
      "\n",
      "Generation 4 - Current best internal CV score: 0.8478752118511078\n",
      "\n",
      "Generation 5 - Current best internal CV score: 0.8478752118511078\n",
      "\n",
      "Generation 6 - Current best internal CV score: 0.8512334442282343\n",
      "\n",
      "Best pipeline: RandomForestClassifier(input_matrix, bootstrap=False, criterion=gini, max_features=0.5, min_samples_leaf=5, min_samples_split=5, n_estimators=100)\n",
      "Imputing missing values in feature set\n",
      "Accuracy 0.9496644295302014\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Load the data\n",
    "data = pd.read_csv('C:/Users/dzyyu/AutoMLTools/AutoML-Tools/TPOT/Datasets/classification_tabular.csv')\n",
    "\n",
    "#Use encoders for datatime and categorical values. TPOT does not supprt encoding.\n",
    "\n",
    "# Use LabelEncoder for datatime variables\n",
    "label_encoder = LabelEncoder()\n",
    "data['date'] = label_encoder.fit_transform(data['date'])\n",
    "data['quarter'] = label_encoder.fit_transform(data['quarter'])\n",
    "data['day'] = label_encoder.fit_transform(data['day'])\n",
    "\n",
    "# Use OneHotEncoder for categorical variables\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "one_hot_encoded = one_hot_encoder.fit_transform(data[['department']])\n",
    "data = data.drop('department', axis=1)\n",
    "data = pd.concat([data, pd.DataFrame(one_hot_encoded.toarray())], axis=1)\n",
    "\n",
    "# Divide the data into input variables and target variable\n",
    "X = data.drop('actual_productivity', axis=1)\n",
    "X.columns = X.columns.astype(str)\n",
    "y = data['actual_productivity']\n",
    "\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25)\n",
    "\n",
    "# Create a tpot object with a few generations and population size for simplicity. Use suggested from TPOT parameters.\n",
    "tpot = TPOTClassifier(generations=6, population_size=20, cv=5,random_state=42, verbosity=2)\n",
    "\n",
    "# Fit the tpot model on the training data\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "#print score of the suggested pipeline model\n",
    "accuracy=tpot.score(X_train, y_train)\n",
    "print(\"Accuracy\", accuracy)\n",
    "\n",
    "#save pipeline with algorithm for future preduction\n",
    "tpot.export('C:/Users/dzyyu/Arbeit/MasterArbeit/MASTERARBEIT/Datasets/classification/tpot_exported_pipeline.py')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "122c9ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1\n",
      " 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1\n",
      " 1 1 1]\n",
      "Accuracy: 0.5719063545150501\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('C:/Users/dzyyu/AutoMLTools/AutoML-Tools/TPOT/Datasets/classification_tabular.csv', sep=',')  # replace ',' with your actual column separator\n",
    "\n",
    "\n",
    "# Use LabelEncoder for datatime variables\n",
    "label_encoder = LabelEncoder()\n",
    "data['date'] = label_encoder.fit_transform(data['date'])\n",
    "data['quarter'] = label_encoder.fit_transform(data['quarter'])\n",
    "data['day'] = label_encoder.fit_transform(data['day'])\n",
    "\n",
    "# Use OneHotEncoder for categorical variables\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "one_hot_encoded = one_hot_encoder.fit_transform(data[['department']])\n",
    "data = data.drop('department', axis=1)\n",
    "data = pd.concat([data, pd.DataFrame(one_hot_encoded.toarray())], axis=1)\n",
    "\n",
    "# Divide the data into input variables and target variable\n",
    "X = data.drop('actual_productivity', axis=1)\n",
    "X.columns = X.columns.astype(str)\n",
    "y = data['actual_productivity']\n",
    "\n",
    "# next lines copied from TPOT exported pipeline. Lines created by TPOT\n",
    "\n",
    "# Split your data into training and test sets\n",
    "training_features, testing_features, training_target, testing_target = train_test_split(X, y,train_size=0.75, test_size=0.25, random_state=42)\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "imputer.fit(training_features)\n",
    "training_features = imputer.transform(training_features)\n",
    "testing_features = imputer.transform(testing_features)\n",
    "\n",
    "# Instantiate and train the pipeline. Use the best algorithm choosen by TPOT\n",
    "exported_pipeline = RandomForestClassifier(bootstrap=False, criterion=\"gini\", max_features=0.5, min_samples_leaf=5, min_samples_split=5, n_estimators=100)\n",
    "exported_pipeline.fit(training_features, training_target)\n",
    "\n",
    "# Make predictions\n",
    "results = exported_pipeline.predict(testing_features)\n",
    "\n",
    "# If you wish to see the predictions, you can print them:\n",
    "accuracy = accuracy_score(testing_target, results)\n",
    "print(\"Predictions:\", results)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8148c4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python38\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but GaussianNB was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# using TPOT pipeline to train the model and make prediction for one instance It was used pipeline from TPOT.\n",
    "#Label and one-hot encoding was done manually.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# NOTE: Make sure that the outcome column is labeled 'target' in the data file\n",
    "data = pd.read_csv('C:/Users/dzyyu/AutoMLTools/AutoML-Tools/TPOT/Datasets/classification_tabular.csv', sep=',')\n",
    "\n",
    "# Use LabelEncoder for datatime variables\n",
    "label_encoder = LabelEncoder()\n",
    "data['date'] = label_encoder.fit_transform(data['date'])\n",
    "data['quarter'] = label_encoder.fit_transform(data['quarter'])\n",
    "data['day'] = label_encoder.fit_transform(data['day'])\n",
    "\n",
    "# Use OneHotEncoder for categorical variables\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "one_hot_encoded = one_hot_encoder.fit_transform(data[['department']])\n",
    "data = data.drop('department', axis=1)\n",
    "data = pd.concat([data, pd.DataFrame(one_hot_encoded.toarray())], axis=1)\n",
    "\n",
    "# Divide the data into input variables and target variable\n",
    "X = data.drop('actual_productivity', axis=1)\n",
    "X.columns = X.columns.astype(str)\n",
    "y = data['actual_productivity']\n",
    "\n",
    "# Split your data into training and test sets\n",
    "training_features, testing_features, training_target, testing_target = train_test_split(X, y, train_size=0.75, test_size=0.25, random_state=42)\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "imputer.fit(training_features)\n",
    "training_features = imputer.transform(training_features)\n",
    "testing_features = imputer.transform(testing_features)\n",
    "\n",
    "\n",
    "# Instantiate and train the pipeline\n",
    "exported_pipeline = RandomForestClassifier(bootstrap=False, criterion=\"gini\", max_features=0.5, min_samples_leaf=5, min_samples_split=5, n_estimators=100)\n",
    "exported_pipeline.fit(training_features, training_target)\n",
    "\n",
    "single_data = pd.DataFrame([[\"01/01/2015\",\"Quarter1\",\"sweing\",\"Thursday\",10,0.75,19.31,578.0,6480,45,0.0,0,0,54.0]], \n",
    "                           columns=['date', 'quarter', 'department', 'day', 'team', 'targeted_productivity', 'smv', \n",
    "                                    'wip', 'over_time', 'incentive', 'idle_time', 'idle_men', 'no_of_style_change', \n",
    "                                    'no_of_workers'])\n",
    "\n",
    "# Apply the same transformation steps as for training set\n",
    "# Use the previously fitted LabelEncoder for datetime variables\n",
    "single_data['date'] = label_encoder.fit_transform(single_data['date'])\n",
    "single_data['quarter'] = label_encoder.fit_transform(single_data['quarter'])\n",
    "single_data['day'] = label_encoder.fit_transform(single_data['day'])\n",
    "\n",
    "# Use the previously fitted OneHotEncoder for categorical variables\n",
    "one_hot_encoded = one_hot_encoder.transform(single_data[['department']]).toarray()\n",
    "\n",
    "# Drop the 'department' column\n",
    "single_data = single_data.drop('department', axis=1)\n",
    "\n",
    "# Add the one-hot encoded columns\n",
    "single_data = pd.concat([single_data, pd.DataFrame(one_hot_encoded)], axis=1)\n",
    "\n",
    "# Convert column names to string type (to match training data)\n",
    "single_data.columns = single_data.columns.astype(str)\n",
    "\n",
    "# Predict using trained model\n",
    "single_result = exported_pipeline.predict(single_data)\n",
    "print(single_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa0f35d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee327304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb4ed95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31063d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7eeedd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16b3737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb7c906",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
